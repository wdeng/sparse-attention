# Dataset configuration
train_datasets:
  - name: "c4"
    path: "allenai/c4"
    split: "train"
    weight: 0.7
  - name: "pile"
    path: "EleutherAI/pile"
    split: "train"
    weight: 0.3
    
eval_datasets:
  - name: "c4"
    path: "allenai/c4"
    split: "validation"
    
# Tokenizer
tokenizer:
  vocab_size: ${model.vocab_size}
  model_path: null  # Will be trained if not provided
  special_tokens:
    pad_token: "[PAD]"
    unk_token: "[UNK]"
    bos_token: "[BOS]"
    eos_token: "[EOS]"
    fill_token: "[FILL]"
    
# Sequence handling
max_length: 8192
target_length: 4096
buffer_size: 1000

# Data loading
streaming: true
shuffle_buffer: 10000
num_proc: 4

# Sequence packing
packing:
  enabled: true
  max_sequences: 8
  pad_to_multiple: 8
  
# Data preprocessing
preprocessing:
  lower_case: false
  clean_text: true
  remove_html: true
  max_line_length: 1000
  
# Data augmentation
augmentation:
  enabled: false
  techniques: []  # List of augmentation techniques
  probability: 0.0
  
# Caching
caching:
  enabled: true
  directory: "cache"
  max_size_gb: 100 