# Training configuration
batch_size: 32
gradient_accumulation_steps: 8
max_steps: 100000
steps_per_epoch: 1000
max_grad_norm: 1.0

# Optimizer
optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-4
  betas: [0.9, 0.999]
  eps: 1e-8
  weight_decay: 0.01
  
# Learning rate scheduler
scheduler:
  _target_: training.optim.CosineSchedulerWithWarmup
  warmup_steps: 2000
  max_steps: ${..max_steps}
  min_lr: 1e-5
  
# Loss scaling for mixed precision
loss_scaling:
  initial_scale: 2**16
  growth_interval: 1000
  backoff_factor: 0.5
  growth_factor: 2.0
  
# Gradient checkpointing
gradient_checkpointing:
  enabled: true
  offload_to_cpu: false
  
# FSDP configuration
fsdp:
  sharding_strategy: "FULL_SHARD"  # [FULL_SHARD, SHARD_GRAD_OP, NO_SHARD]
  backward_prefetch: "BACKWARD_PRE"  # [BACKWARD_PRE, BACKWARD_POST]
  activation_checkpointing: true
  cpu_offload: true
  
# Data loading
dataloader:
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  
# Evaluation
eval:
  eval_steps: 1000
  save_predictions: true
  metrics: ["loss", "perplexity", "accuracy"] 